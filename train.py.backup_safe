import os
import time
import random
import argparse

import torch
import torch.distributed as dist

from tqdm import tqdm
from options.options import parse
from data.dataset_tools.datapipeline import *
from archs import *
from losses import *
from data import *
from utils.utils import create_path_models, set_random_seed
from tools.trainer import train_model
from tools.tester import eval_model


# Add this function near the top of train.py (after imports)
def safe_save_checkpoint(model, optim, scheduler, metrics_eval, metrics_train, path, global_rank):
    """Save checkpoints in organized structure"""
    import os
    import torch
    import time
    
    if global_rank != 0:
        return metrics_eval.get('valid_psnr', 0)
    
    epoch = metrics_train.get('epoch', 0)
    psnr = metrics_eval.get('valid_psnr', 0)
    timestamp = time.strftime("%Y%m%d_%H%M%S")
    
    # Ensure base directory exists
    base_path = path if not path.endswith(('.pt', '.pth')) else os.path.dirname(path)
    if not base_path:
        base_path = "./checkpoints"
    os.makedirs(base_path, exist_ok=True)
    
    # 1. Save to by_epoch/ folder (every 10 epochs)
    if epoch % 10 == 0 or epoch == metrics_train.get('total_epochs', 1000) - 1:
        epoch_dir = os.path.join(base_path, "by_epoch")
        os.makedirs(epoch_dir, exist_ok=True)
        epoch_path = os.path.join(epoch_dir, f"epoch_{epoch:03d}_psnr_{psnr:.2f}.pt")
        
        torch.save({
            'epoch': epoch,
            'model_state_dict': model.state_dict(),
            'optimizer_state_dict': optim.state_dict(),
            'scheduler_state_dict': scheduler.state_dict() if scheduler else None,
            'metrics_eval': metrics_eval,
            'metrics_train': metrics_train,
            'timestamp': timestamp,
            'psnr': psnr
        }, epoch_path)
        
        print(f"ğŸ“ Epoch checkpoint saved: {epoch_path}")
    
    # 2. Save latest model
    latest_dir = os.path.join(base_path, "latest")
    os.makedirs(latest_dir, exist_ok=True)
    latest_path = os.path.join(latest_dir, "latest_checkpoint.pt")
    
    torch.save({
        'epoch': epoch,
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': optim.state_dict(),
        'scheduler_state_dict': scheduler.state_dict() if scheduler else None,
        'metrics_eval': metrics_eval,
        'metrics_train': metrics_train,
        'timestamp': timestamp,
        'psnr': psnr
    }, latest_path)
    
    # 3. Track and save best model
    best_dir = os.path.join(base_path, "best_models")
    os.makedirs(best_dir, exist_ok=True)
    
    # Check current best PSNR
    best_psnr_file = os.path.join(best_dir, "best_psnr.txt")
    best_psnr = 0
    
    if os.path.exists(best_psnr_file):
        try:
            with open(best_psnr_file, 'r') as f:
                best_psnr = float(f.read().strip())
        except:
            pass
    
    # Save if this is the best model
    if psnr > best_psnr:
        # Update best PSNR file
        with open(best_psnr_file, 'w') as f:
            f.write(str(psnr))
        
        # Save best model
        best_path = os.path.join(best_dir, f"best_psnr_{psnr:.2f}_epoch_{epoch}.pt")
        torch.save({
            'epoch': epoch,
            'model_state_dict': model.state_dict(),
            'optimizer_state_dict': optim.state_dict(),
            'metrics_eval': metrics_eval,
            'metrics_train': metrics_train,
            'timestamp': timestamp,
            'psnr': psnr,
            'is_best': True
        }, best_path)
        
        print(f"ğŸ† NEW BEST MODEL! PSNR: {psnr:.4f} saved to: {best_path}")
    
    return psnr
def run_model(opt, PATH_RESUME, PATH_SAVE):
    """
    ä¸»è®­ç»ƒå‡½æ•°ï¼Œå¤„ç†å•å¡/å¤šå¡åˆå§‹åŒ–å¹¶å¯åŠ¨è®­ç»ƒå¾ªç¯ã€‚
    """
    #global opt, PATH_RESUME, PATH_SAVE, final_score  # â† Explicitly declare globals
    # Use opt, PATH_RESUME, PATH_SAVE directly
    #seed = opt['datasets'].get('seed')
    final_score = 0.

    # ========== 1. åˆ†å¸ƒå¼ç¯å¢ƒåˆå§‹åŒ– (å•å¡/å¤šå¡è‡ªé€‚åº”) ==========
    # åˆå§‹åŒ–å…³é”®å˜é‡ï¼ˆæ— è®ºå•å¡å¤šå¡éƒ½ä¼šç”¨åˆ°ï¼‰
    world_size = 1
    global_rank = 0
    local_rank = 0


    
    # æ£€æŸ¥æ˜¯å¦é€šè¿‡ç¯å¢ƒå˜é‡å¯ç”¨äº†åˆ†å¸ƒå¼è®­ç»ƒ
    env_world_size = int(os.getenv('WORLD_SIZE', '1'))
    
    if env_world_size > 1 and dist.is_available():
        # ---------- å¤šGPUåˆ†å¸ƒå¼æ¨¡å¼ ----------
        try:
            dist.init_process_group(backend='nccl')
            world_size = dist.get_world_size()
            global_rank = dist.get_rank()
            local_rank = int(os.environ.get('LOCAL_RANK', '0'))
            print(f"[åˆ†å¸ƒå¼æ¨¡å¼] å…¨å±€è¿›ç¨‹ {global_rank} / å…± {world_size}, æœ¬åœ°è®¾å¤‡ {local_rank}")
        except Exception as e:
            print(f"[è­¦å‘Š] åˆ†å¸ƒå¼åˆå§‹åŒ–å¤±è´¥ï¼Œå°†å›é€€åˆ°å•å¡æ¨¡å¼ã€‚é”™è¯¯: {e}")
            # å¤±è´¥åå›é€€åˆ°å•å¡æ¨¡å¼
            world_size = 1
            global_rank = 0
            local_rank = 0
    else:
        # ---------- å•GPUæ¨¡å¼ (ä½ çš„ä½¿ç”¨åœºæ™¯) ----------
        world_size = 1
        global_rank = 0
        local_rank = 0
        print("[å•å¡æ¨¡å¼] åˆ†å¸ƒå¼è®­ç»ƒå·²ç¦ç”¨ã€‚")
    
    # æ˜ç¡®è®¾ç½®ç¯å¢ƒå˜é‡ï¼Œé˜²æ­¢ä»£ç å…¶ä»–éƒ¨åˆ†å› æ£€æŸ¥è€ŒæŠ¥é”™
    os.environ['RANK'] = str(global_rank)
    os.environ['WORLD_SIZE'] = str(world_size)
    os.environ['LOCAL_RANK'] = str(local_rank)

     # ====== æ–°å¢ï¼šä¸ºDDPåˆå§‹åŒ–ä¸€ä¸ªå•æœºâ€œåˆ†å¸ƒå¼â€ç¯å¢ƒ ======
    if not dist.is_initialized():
        # å³ä½¿å•å¡ï¼Œä¹Ÿä¸ºå…¼å®¹DDPè€Œåˆå§‹åŒ–è¿›ç¨‹ç»„
        # ä½¿ç”¨'gloo'æˆ–'ncccl'åç«¯ï¼Œglooåœ¨Windowsä¸Šæ›´å…¼å®¹
        try:
            dist.init_process_group(backend='gloo', rank=global_rank, world_size=world_size,
                                     init_method='env://')
            print(f"[å…¼å®¹å±‚] å·²åˆå§‹åŒ–å•æœºè¿›ç¨‹ç»„ä»¥é€‚é…DDPåŒ…è£…å™¨ã€‚")
        except Exception as e:
            print(f"[è­¦å‘Š] è¿›ç¨‹ç»„åˆå§‹åŒ–å¤±è´¥: {e}. å°è¯•ç»§ç»­...")
    # =============================================




    # ========== 2. è®¾ç½®å½“å‰è¿›ç¨‹ä½¿ç”¨çš„GPU ==========
    torch.cuda.set_device(local_rank)
    print(f'è¿›ç¨‹çŠ¶æ€: å…¨å±€æ’å {global_rank} / æ€»è¿›ç¨‹æ•° {world_size}, æœ¬åœ°GPUç¼–å· {local_rank}')

    # ========== 3. è®¾ç½®éšæœºç§å­ (ä¿è¯å®éªŒå¯å¤ç°) ==========
    seed = opt['datasets'].get('seed')
    if seed is None:
        seed = random.randint(1, 10000)
        opt['datasets']['seed'] = seed
    set_random_seed(seed + global_rank)  # ä¸ºä¸åŒè¿›ç¨‹è®¾ç½®ä¸åŒç§å­

    # ========== 4. åˆ›å»ºæ¨¡å‹ã€ç»Ÿè®¡è®¡ç®—é‡å‚æ•°é‡ ==========
    model, macs, params = create_model(opt['network'], local_rank=local_rank, global_rank=global_rank)
    opt['macs'] = macs
    opt['params'] = params
    opt['Total_GPUs'] = world_size  # å°†GPUæ•°é‡è®°å½•åˆ°é…ç½®ä¸­

    # ========== 5. å®šä¹‰ä¼˜åŒ–å™¨å’Œå­¦ä¹ ç‡è°ƒåº¦å™¨ ==========
    optim, scheduler = create_optim_scheduler(opt['train'], model)

    # ========== 6. åŠ è½½æ£€æŸ¥ç‚¹ (å¦‚æœéœ€è¦æ¢å¤è®­ç»ƒ) ==========
    model, optim, scheduler, _ = resume_model(model, optim, scheduler,
                                               path_model=PATH_RESUME,
                                               local_rank=local_rank,
                                               global_rank=global_rank,
                                               resume=opt['network'].get('resume_training', False))

    # ========== 7. ä¸»è®­ç»ƒå¾ªç¯ ==========
    for step in range(opt['train']['STEPS']):
        total_steps = opt['train']['STEPS']
        if global_rank == 0:
            print(f'\n--------------- é˜¶æ®µ {step + 1} / {total_steps} ---------------')

        # 7.1 ä¸ºå½“å‰è®­ç»ƒé˜¶æ®µåˆ›å»ºæ•°æ®åŠ è½½å™¨
        train_loader, test_loader, samplers = create_data(global_rank,
                                                           world_size=world_size,
                                                           opt=opt['datasets'],
                                                           step=step)

        # 7.2 ä¸ºå½“å‰é˜¶æ®µåˆ›å»ºæŸå¤±å‡½æ•°
        all_losses = create_loss(opt['train'], step=step,
                                 local_rank=local_rank,
                                 global_rank=global_rank)

        final_score = 0  # é‡ç½®æ¯ä¸ªé˜¶æ®µçš„æœ€ä½³åˆ†æ•°

        # 7.3 è¿›åº¦æ¡ (ä»…ä¸»è¿›ç¨‹æ˜¾ç¤º)
        if global_rank == 0:
            pbar = tqdm(total=opt['train']['epochs'][step])

        # 7.4 è¿­ä»£æ¯ä¸€ä¸ªEpoch
        for epoch in range(opt['train']['epochs'][step]):
            epoch_start_time = time.time()
            metrics_train = {'epoch': epoch, 'final_score': final_score}
            metrics_eval = {}

            # åœ¨æ¯ä¸ªEpochå¼€å§‹æ—¶æ‰“ä¹±è®­ç»ƒæ•°æ®é¡ºåº
            shuffle_sampler(samplers, epoch)

            # ---------- è®­ç»ƒé˜¶æ®µ ----------
            model.train()
            model, optim, metrics_train = train_model(model, optim, all_losses,
                                                      train_loader, metrics_train,
                                                      local_rank=local_rank)

            # ---------- è¯„ä¼°é˜¶æ®µ eval phase ----------
            eval_this_epoch = (epoch % opt['train']['eval_freq'] == 0) or (epoch == opt['train']['epochs'][step] - 1)
            if eval_this_epoch:
                model.eval()
                metrics_eval = eval_model(model, test_loader, metrics_eval,
                                          local_rank=local_rank,
                                          world_size=world_size)

                # 7.5 ä¸»è¿›ç¨‹æ‰“å°è¯„ä¼°ç»“æœ
                if global_rank == 0:
                    # æ‰“å°è€—æ—¶
                    epoch_time = time.time() - epoch_start_time
                    print(f"Epoch {epoch + 1}/{opt['train']['epochs'][step]} è€—æ—¶: {epoch_time:.3f}s")

                    # æ‰“å°è¯„ä¼°æŒ‡æ ‡ (PSNR, SSIM, LPIPS)
                    if isinstance(next(iter(metrics_eval.values())), dict):
                        # å¦‚æœæœ‰å¤šä¸ªè¯„ä¼°æŒ‡æ ‡é›†
                        for key, metric_eval in metrics_eval.items():
                            print(f"  \t{key} --- PSNR: {metric_eval['valid_psnr']:.4f}, "
                                  f"SSIM: {metric_eval['valid_ssim']:.4f}, "
                                  f"LPIPS: {metric_eval['valid_lpips']:.4f}")
                    else:
                        # åªæœ‰ä¸€ä¸ªè¯„ä¼°æŒ‡æ ‡é›†
                        dataset_name = opt['datasets']['name']
                        print(f"  \t{dataset_name} --- PSNR: {metrics_eval['valid_psnr']:.4f}, "
                              f"SSIM: {metrics_eval['valid_ssim']:.4f}, "
                              f"LPIPS: {metrics_eval['valid_lpips']:.4f}")

                    # æ›´æ–°è¿›åº¦æ¡
                    pbar.update(1)

                # 7.6 ä¸»è¿›ç¨‹ä¿å­˜æ£€æŸ¥ç‚¹
                if global_rank == 0:
                    """
                    final_score = save_checkpoint(model, optim, scheduler,
                                                  metrics_eval=metrics_eval,
                                                  metrics_train=metrics_train,
                                                  paths=PATH_SAVE,
                                                  global_rank=global_rank)
                    """
                    final_score = safe_save_checkpoint(model, optim, scheduler, 
                                   metrics_eval=metrics_eval, 
                                   metrics_train=metrics_train, 
                                   path=PATH_SAVE, 
                                   global_rank=global_rank)
            # 7.7 æ›´æ–°å­¦ä¹ ç‡
            scheduler.step()

        # 7.8 å…³é—­å½“å‰é˜¶æ®µçš„è¿›åº¦æ¡
        if global_rank == 0:
            pbar.close()

if __name__ == '__main__':
    # ==================== å‘½ä»¤è¡Œå‚æ•°è§£æ ====================
    parser = argparse.ArgumentParser(description="Script for train")
    parser.add_argument('-p', '--config', type=str, default='./options/train/RSBlur.yml', help='Config file for training')
    args = parser.parse_args()

    # ==================== åŠ è½½é…ç½®æ–‡ä»¶ ====================
    opt = parse(args.config)

    # ==================== æ¨¡å‹ä¿å­˜è·¯å¾„ ====================
    PATH_RESUME, PATH_SAVE = create_path_models(opt['save'])
    final_score = 0.
    
    # Pass variables to function
    run_model(opt, PATH_RESUME, PATH_SAVE)