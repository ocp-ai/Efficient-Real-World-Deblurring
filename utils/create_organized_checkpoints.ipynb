{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90ca8831",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ Setting up organized checkpoint system...\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Setup and Imports\n",
    "import os\n",
    "import shutil\n",
    "import datetime\n",
    "import json\n",
    "from pathlib import Path\n",
    "import torch\n",
    "\n",
    "print(\"üìÅ Setting up organized checkpoint system...\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dddad52e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Not found: ./models/My_Microscope_NAFNet_C16_L14.ptMy_Microscope_NAFNet_C16_L14.pt\n",
      "‚úì Not found: ./models/My_Microscope_NAFNet_C16_L14.pt\n",
      "‚úì Not found: ./models/My_Microscope_NAFNet_C16_L14\n",
      "‚úì Not found: ./checkpoints/My_Microscope_NAFNet_C16_L14.ptMy_Microscope_NAFNet_C16_L14.pt\n",
      "‚úì Not found: ./checkpoints/My_Microscope_NAFNet_C16_L14.pt\n",
      "‚úì Not found: ./checkpoints/My_Microscope_NAFNet_C16_L14\n",
      "‚úì Not found: ./models/checkpoints\n",
      "‚úì Not found: ./checkpoints/checkpoints\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Clean Up Crazy Paths\n",
    "def clean_crazy_paths():\n",
    "    \"\"\"Clean up all the crazy checkpoint paths\"\"\"\n",
    "    crazy_paths = [\n",
    "        \"./models/My_Microscope_NAFNet_C16_L14.ptMy_Microscope_NAFNet_C16_L14.pt\",\n",
    "        \"./models/My_Microscope_NAFNet_C16_L14.pt\",\n",
    "        \"./models/My_Microscope_NAFNet_C16_L14\",\n",
    "        \"./checkpoints/My_Microscope_NAFNet_C16_L14.ptMy_Microscope_NAFNet_C16_L14.pt\",\n",
    "        \"./checkpoints/My_Microscope_NAFNet_C16_L14.pt\",\n",
    "        \"./checkpoints/My_Microscope_NAFNet_C16_L14\",\n",
    "        \"./models/checkpoints\",\n",
    "        \"./checkpoints/checkpoints\"\n",
    "    ]\n",
    "    \n",
    "    cleaned = []\n",
    "    for path in crazy_paths:\n",
    "        if os.path.exists(path):\n",
    "            try:\n",
    "                if os.path.isfile(path):\n",
    "                    os.remove(path)\n",
    "                    cleaned.append(f\"‚úÖ Removed file: {path}\")\n",
    "                else:\n",
    "                    shutil.rmtree(path)\n",
    "                    cleaned.append(f\"‚úÖ Removed directory: {path}\")\n",
    "            except Exception as e:\n",
    "                cleaned.append(f\"‚ùå Could not remove {path}: {e}\")\n",
    "        else:\n",
    "            cleaned.append(f\"‚úì Not found: {path}\")\n",
    "    \n",
    "    return cleaned\n",
    "\n",
    "# Run cleanup\n",
    "results = clean_crazy_paths()\n",
    "for result in results:\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26e1580d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìÇ Creating directory structure...\n",
      "üìÅ checkpoints\\by_epoch/ - Checkpoints saved every N epochs\n",
      "üìÅ checkpoints\\best_models/ - Best performing models (auto-saved)\n",
      "üìÅ checkpoints\\latest/ - Always the latest checkpoint\n",
      "üìÅ checkpoints\\configs/ - Training configurations\n",
      "üìÅ checkpoints\\logs/ - Training logs and metrics\n",
      "üìÅ checkpoints\\visualizations/ - Sample outputs and comparisons\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Create Organized Directory Structure\n",
    "def create_checkpoint_structure():\n",
    "    \"\"\"Create organized checkpoint directory structure\"\"\"\n",
    "    checkpoint_base = Path(\"./checkpoints\")\n",
    "    \n",
    "    # Main directories\n",
    "    directories = {\n",
    "        \"by_epoch\": \"Checkpoints saved every N epochs\",\n",
    "        \"best_models\": \"Best performing models (auto-saved)\",\n",
    "        \"latest\": \"Always the latest checkpoint\",\n",
    "        \"configs\": \"Training configurations\",\n",
    "        \"logs\": \"Training logs and metrics\",\n",
    "        \"visualizations\": \"Sample outputs and comparisons\"\n",
    "    }\n",
    "    \n",
    "    created = []\n",
    "    for dir_name, description in directories.items():\n",
    "        dir_path = checkpoint_base / dir_name\n",
    "        dir_path.mkdir(parents=True, exist_ok=True)\n",
    "        created.append(f\"üìÅ {dir_path}/ - {description}\")\n",
    "    \n",
    "    # Subdirectories for better organization\n",
    "    (checkpoint_base / \"by_epoch\" / \"every_10\").mkdir(exist_ok=True)\n",
    "    (checkpoint_base / \"by_epoch\" / \"every_50\").mkdir(exist_ok=True)\n",
    "    (checkpoint_base / \"by_epoch\" / \"milestones\").mkdir(exist_ok=True)\n",
    "    \n",
    "    (checkpoint_base / \"best_models\" / \"by_psnr\").mkdir(exist_ok=True)\n",
    "    (checkpoint_base / \"best_models\" / \"by_ssim\").mkdir(exist_ok=True)\n",
    "    (checkpoint_base / \"best_models\" / \"by_lpips\").mkdir(exist_ok=True)\n",
    "    \n",
    "    return created\n",
    "\n",
    "# Create structure\n",
    "print(\"\\nüìÇ Creating directory structure...\")\n",
    "created_dirs = create_checkpoint_structure()\n",
    "for dir_info in created_dirs:\n",
    "    print(dir_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e56b9f93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìù Creating metadata files...\n",
      "‚úÖ Created: checkpoints\\README.md\n",
      "‚úÖ Created: checkpoints\\metadata.json\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Create Metadata and README Files\n",
    "def create_metadata_files():\n",
    "    \"\"\"Create metadata and documentation files\"\"\"\n",
    "    checkpoint_base = Path(\"./checkpoints\")\n",
    "    \n",
    "    # Create README\n",
    "    readme_content = \"\"\"# CHECKPOINTS ORGANIZATION\n",
    "\n",
    "## Directory Structure\n",
    "- `by_epoch/`          - Checkpoints saved at regular intervals\n",
    "  - `every_10/`        - Every 10 epochs\n",
    "  - `every_50/`        - Every 50 epochs  \n",
    "  - `milestones/`      - Important milestones (epoch 1, 50, 100, etc.)\n",
    "- `best_models/`       - Best performing models\n",
    "  - `by_psnr/`         - Best PSNR models\n",
    "  - `by_ssim/`         - Best SSIM models\n",
    "  - `by_lpips/`        - Best LPIPS models\n",
    "- `latest/`            - Always the latest model\n",
    "- `configs/`           - Training configurations\n",
    "- `logs/`              - Training logs and metrics\n",
    "- `visualizations/`    - Sample outputs\n",
    "\n",
    "## Naming Convention\n",
    "- Epoch checkpoints: `epoch_XXX_psnr_YY.YY_ssim_0.ZZZ_lpips_0.AAA.pt`\n",
    "- Best models: `best_psnr_YY.YY_epoch_XXX.pt`\n",
    "- Latest: `latest_checkpoint.pt`\n",
    "\n",
    "## Usage\n",
    "- Resume training: Use any checkpoint from `by_epoch/` or `latest/`\n",
    "- Final deployment: Use best model from `best_models/by_psnr/`\n",
    "- Analysis: Check `logs/` for training history\n",
    "\"\"\"\n",
    "    \n",
    "    readme_path = checkpoint_base / \"README.md\"\n",
    "    readme_path.write_text(readme_content)\n",
    "    \n",
    "    # Create metadata JSON\n",
    "    metadata = {\n",
    "        \"project\": \"Microscope Deblurring with NAFNet\",\n",
    "        \"model\": \"NAFNet-C16-L14\",\n",
    "        \"created\": datetime.datetime.now().isoformat(),\n",
    "        \"author\": \"Your Name\",\n",
    "        \"description\": \"Real-world microscope image deblurring\",\n",
    "        \"dataset\": \"Synthetic RSBlur format\",\n",
    "        \"total_epochs\": 100,\n",
    "        \"checkpoint_schedule\": {\n",
    "            \"every_n_epochs\": 10,\n",
    "            \"save_best\": True,\n",
    "            \"keep_last_n\": 5\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    metadata_path = checkpoint_base / \"metadata.json\"\n",
    "    with open(metadata_path, 'w') as f:\n",
    "        json.dump(metadata, f, indent=2)\n",
    "    \n",
    "    return [str(readme_path), str(metadata_path)]\n",
    "\n",
    "print(\"\\nüìù Creating metadata files...\")\n",
    "metadata_files = create_metadata_files()\n",
    "for file in metadata_files:\n",
    "    print(f\"‚úÖ Created: {file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b8f81928",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚öôÔ∏è Updating YAML configuration...\n",
      "‚úÖ Updated YAML path_save\n",
      "‚úÖ Updated checkpoint frequency\n",
      "‚úÖ Added checkpoint configuration\n",
      "‚úÖ Updated: options\\train\\RSBlur.yml\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Update YAML Configuration\n",
    "def update_yaml_config():\n",
    "    \"\"\"Update the training YAML configuration\"\"\"\n",
    "    yaml_path = Path(\"./options/train/RSBlur.yml\")\n",
    "    \n",
    "    if not yaml_path.exists():\n",
    "        print(f\"‚ùå YAML not found: {yaml_path}\")\n",
    "        return None\n",
    "    \n",
    "    with open(yaml_path, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "    \n",
    "    new_lines = []\n",
    "    updated = False\n",
    "    \n",
    "    for line in lines:\n",
    "        stripped = line.strip()\n",
    "        if stripped.startswith('path_save:'):\n",
    "            # Replace with organized directory structure\n",
    "            new_lines.append('  path_save: ./checkpoints/latest/model.pt  # Latest checkpoint\\n')\n",
    "            updated = True\n",
    "            print(\"‚úÖ Updated YAML path_save\")\n",
    "        elif stripped.startswith('save_checkpoint_freq:'):\n",
    "            # Ensure checkpoints are saved regularly\n",
    "            new_lines.append('  save_checkpoint_freq: 1000  # Save every 1000 iterations\\n')\n",
    "            updated = True\n",
    "            print(\"‚úÖ Updated checkpoint frequency\")\n",
    "        else:\n",
    "            new_lines.append(line)\n",
    "    \n",
    "    # Add checkpoint configuration if not present\n",
    "    if not any('checkpoint_config:' in line for line in new_lines):\n",
    "        new_lines.append('\\n# Checkpoint Configuration\\n')\n",
    "        new_lines.append('checkpoint_config:\\n')\n",
    "        new_lines.append('  save_every_n_epochs: 10\\n')\n",
    "        new_lines.append('  keep_best_n_models: 3\\n')\n",
    "        new_lines.append('  metrics_to_track: [psnr, ssim, lpips]\\n')\n",
    "        new_lines.append('  auto_cleanup: true\\n')\n",
    "        print(\"‚úÖ Added checkpoint configuration\")\n",
    "    \n",
    "    # Write back\n",
    "    with open(yaml_path, 'w', encoding='utf-8') as f:\n",
    "        f.writelines(new_lines)\n",
    "    \n",
    "    return str(yaml_path) if updated else None\n",
    "\n",
    "print(\"\\n‚öôÔ∏è Updating YAML configuration...\")\n",
    "yaml_file = update_yaml_config()\n",
    "if yaml_file:\n",
    "    print(f\"‚úÖ Updated: {yaml_file}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è YAML already up to date or not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f21f9f57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîÑ Initializing Checkpoint Manager...\n",
      "‚úÖ Checkpoint Manager ready!\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Create Checkpoint Manager Class\n",
    "class CheckpointManager:\n",
    "    \"\"\"Manage organized checkpoint saving and loading\"\"\"\n",
    "    \n",
    "    def __init__(self, base_dir=\"./checkpoints\"):\n",
    "        self.base_dir = Path(base_dir)\n",
    "        self.base_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Track best metrics\n",
    "        self.best_metrics = {\n",
    "            'psnr': {'value': 0, 'epoch': 0, 'path': None},\n",
    "            'ssim': {'value': 0, 'epoch': 0, 'path': None},\n",
    "            'lpips': {'value': float('inf'), 'epoch': 0, 'path': None}\n",
    "        }\n",
    "        \n",
    "        # Load existing best metrics if available\n",
    "        self._load_best_metrics()\n",
    "    \n",
    "    def _load_best_metrics(self):\n",
    "        \"\"\"Load previously saved best metrics\"\"\"\n",
    "        metrics_file = self.base_dir / \"best_metrics.json\"\n",
    "        if metrics_file.exists():\n",
    "            try:\n",
    "                with open(metrics_file, 'r') as f:\n",
    "                    saved = json.load(f)\n",
    "                    for key in self.best_metrics:\n",
    "                        if key in saved:\n",
    "                            self.best_metrics[key] = saved[key]\n",
    "                print(\"üìä Loaded previous best metrics\")\n",
    "            except:\n",
    "                print(\"‚ö†Ô∏è Could not load previous best metrics\")\n",
    "    \n",
    "    def _save_best_metrics(self):\n",
    "        \"\"\"Save current best metrics to file\"\"\"\n",
    "        metrics_file = self.base_dir / \"best_metrics.json\"\n",
    "        with open(metrics_file, 'w') as f:\n",
    "            json.dump(self.best_metrics, f, indent=2)\n",
    "    \n",
    "    def save_checkpoint(self, model, optimizer, scheduler, epoch, metrics, \n",
    "                       is_best=False, is_latest=True):\n",
    "        \"\"\"\n",
    "        Save checkpoint with organized structure\n",
    "        \n",
    "        Args:\n",
    "            model: The neural network model\n",
    "            optimizer: The optimizer\n",
    "            scheduler: Learning rate scheduler\n",
    "            epoch: Current epoch number\n",
    "            metrics: Dictionary of metrics e.g., {'psnr': 25.5, 'ssim': 0.8}\n",
    "            is_best: Whether this is the best model so far\n",
    "            is_latest: Whether this is the latest model\n",
    "        \"\"\"\n",
    "        timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        \n",
    "        # Create checkpoint data\n",
    "        checkpoint = {\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict() if scheduler else None,\n",
    "            'metrics': metrics,\n",
    "            'timestamp': timestamp,\n",
    "            'is_best': is_best,\n",
    "            'is_latest': is_latest\n",
    "        }\n",
    "        \n",
    "        # 1. Save to by_epoch directory (every 10 epochs)\n",
    "        if epoch % 10 == 0 or epoch in [1, 50, 100]:\n",
    "            subdir = \"every_10\" if epoch % 10 == 0 else \"milestones\"\n",
    "            epoch_dir = self.base_dir / \"by_epoch\" / subdir\n",
    "            epoch_dir.mkdir(parents=True, exist_ok=True)\n",
    "            \n",
    "            # Create descriptive filename\n",
    "            psnr = metrics.get('psnr', 0)\n",
    "            ssim = metrics.get('ssim', 0)\n",
    "            lpips = metrics.get('lpips', 0)\n",
    "            \n",
    "            epoch_filename = f\"epoch_{epoch:03d}_psnr_{psnr:.2f}_ssim_{ssim:.3f}_lpips_{lpips:.3f}.pt\"\n",
    "            epoch_path = epoch_dir / epoch_filename\n",
    "            \n",
    "            torch.save(checkpoint, epoch_path)\n",
    "            print(f\"üìÅ Epoch checkpoint: {epoch_path.relative_to(self.base_dir)}\")\n",
    "        \n",
    "        # 2. Save latest model\n",
    "        if is_latest:\n",
    "            latest_dir = self.base_dir / \"latest\"\n",
    "            latest_dir.mkdir(exist_ok=True)\n",
    "            latest_path = latest_dir / \"latest_checkpoint.pt\"\n",
    "            torch.save(checkpoint, latest_path)\n",
    "            print(f\"üîÑ Latest checkpoint: {latest_path.relative_to(self.base_dir)}\")\n",
    "        \n",
    "        # 3. Check and save best models\n",
    "        updated_bests = []\n",
    "        for metric_name, current_value in metrics.items():\n",
    "            if metric_name in self.best_metrics:\n",
    "                best_info = self.best_metrics[metric_name]\n",
    "                \n",
    "                # Determine if this is better (higher is better for psnr/ssim, lower for lpips)\n",
    "                if metric_name == 'lpips':\n",
    "                    is_better = current_value < best_info['value']\n",
    "                else:\n",
    "                    is_better = current_value > best_info['value']\n",
    "                \n",
    "                if is_better:\n",
    "                    # Update best metrics\n",
    "                    self.best_metrics[metric_name] = {\n",
    "                        'value': current_value,\n",
    "                        'epoch': epoch,\n",
    "                        'path': None  # Will be set after saving\n",
    "                    }\n",
    "                    \n",
    "                    # Save best model\n",
    "                    best_dir = self.base_dir / \"best_models\" / f\"by_{metric_name}\"\n",
    "                    best_dir.mkdir(parents=True, exist_ok=True)\n",
    "                    \n",
    "                    best_filename = f\"best_{metric_name}_{current_value:.4f}_epoch_{epoch}.pt\"\n",
    "                    best_path = best_dir / best_filename\n",
    "                    \n",
    "                    torch.save(checkpoint, best_path)\n",
    "                    \n",
    "                    # Update path in metrics\n",
    "                    self.best_metrics[metric_name]['path'] = str(best_path.relative_to(self.base_dir))\n",
    "                    \n",
    "                    updated_bests.append((metric_name, current_value, best_path))\n",
    "        \n",
    "        # Save updated best metrics\n",
    "        if updated_bests:\n",
    "            self._save_best_metrics()\n",
    "            for metric_name, value, path in updated_bests:\n",
    "                print(f\"üèÜ NEW BEST {metric_name.upper()}: {value:.4f} at {path.relative_to(self.base_dir)}\")\n",
    "        \n",
    "        return checkpoint\n",
    "\n",
    "# Initialize checkpoint manager\n",
    "print(\"\\nüîÑ Initializing Checkpoint Manager...\")\n",
    "checkpoint_manager = CheckpointManager()\n",
    "print(\"‚úÖ Checkpoint Manager ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4485bc2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Creating utility functions...\n",
      " Training log: checkpoints\\logs\\training_log.csv\n",
      " Training log: checkpoints\\logs\\training_log.csv\n",
      " Visualization script: visualize_checkpoints.py\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: Create Utility Functions\n",
    "def create_training_log():\n",
    "    \"\"\"Create training log file\"\"\"\n",
    "    log_dir = Path(\"./checkpoints/logs\")\n",
    "    log_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    log_file = log_dir / \"training_log.csv\"\n",
    "    \n",
    "    # Create CSV header if file doesn't exist\n",
    "    if not log_file.exists():\n",
    "        with open(log_file, 'w') as f:\n",
    "            f.write(\"epoch,timestamp,psnr,ssim,lpips,loss,learning_rate,is_best_psnr,is_best_ssim,is_best_lpips\\\\n\")\n",
    "    \n",
    "    print(f\" Training log: {log_file}\")\n",
    "    return log_file\n",
    "\n",
    "def log_training_step(log_file, epoch, metrics, loss, lr, is_bests):\n",
    "    \"\"\"Log a training step to CSV\"\"\"\n",
    "    timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    \n",
    "    with open(log_file, 'a') as f:\n",
    "        f.write(f\"{epoch},{timestamp},{metrics.get('psnr', 0):.4f},{metrics.get('ssim', 0):.4f},\")\n",
    "        f.write(f\"{metrics.get('lpips', 0):.4f},{loss:.6f},{lr:.6e},\")\n",
    "        f.write(f\"{int(is_bests.get('psnr', False))},{int(is_bests.get('ssim', False))},{int(is_bests.get('lpips', False))}\\\\n\")\n",
    "    \n",
    "    return True\n",
    "\n",
    "def create_visualization_script():\n",
    "    \"\"\"Create script to visualize checkpoint progress\"\"\"\n",
    "    script_content = '''import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "def visualize_training_progress():\n",
    "    \"\"\"Visualize training progress from checkpoints\"\"\"\n",
    "    \n",
    "    # 1. Load training log\n",
    "    log_path = Path(\"./checkpoints/logs/training_log.csv\")\n",
    "    if log_path.exists():\n",
    "        df = pd.read_csv(log_path)\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "        \n",
    "        # Plot PSNR\n",
    "        axes[0, 0].plot(df['epoch'], df['psnr'], 'b-', label='PSNR')\n",
    "        axes[0, 0].scatter(df[df['is_best_psnr']==1]['epoch'], \n",
    "                          df[df['is_best_psnr']==1]['psnr'], \n",
    "                          color='red', s=50, label='Best PSNR')\n",
    "        axes[0, 0].set_xlabel('Epoch')\n",
    "        axes[0, 0].set_ylabel('PSNR')\n",
    "        axes[0, 0].set_title('PSNR Progress')\n",
    "        axes[0, 0].legend()\n",
    "        axes[0, 0].grid(True)\n",
    "        \n",
    "        # Plot SSIM\n",
    "        axes[0, 1].plot(df['epoch'], df['ssim'], 'g-', label='SSIM')\n",
    "        axes[0, 1].scatter(df[df['is_best_ssim']==1]['epoch'], \n",
    "                          df[df['is_best_ssim']==1]['ssim'], \n",
    "                          color='red', s=50, label='Best SSIM')\n",
    "        axes[0, 1].set_xlabel('Epoch')\n",
    "        axes[0, 1].set_ylabel('SSIM')\n",
    "        axes[0, 1].set_title('SSIM Progress')\n",
    "        axes[0, 1].legend()\n",
    "        axes[0, 1].grid(True)\n",
    "        \n",
    "        # Plot LPIPS\n",
    "        axes[1, 0].plot(df['epoch'], df['lpips'], 'r-', label='LPIPS')\n",
    "        axes[1, 0].scatter(df[df['is_best_lpips']==1]['epoch'], \n",
    "                          df[df['is_best_lpips']==1]['lpips'], \n",
    "                          color='green', s=50, label='Best LPIPS')\n",
    "        axes[1, 0].set_xlabel('Epoch')\n",
    "        axes[1, 0].set_ylabel('LPIPS')\n",
    "        axes[1, 0].set_title('LPIPS Progress (lower is better)')\n",
    "        axes[1, 0].legend()\n",
    "        axes[1, 0].grid(True)\n",
    "        \n",
    "        # Plot Loss\n",
    "        axes[1, 1].plot(df['epoch'], df['loss'], 'm-', label='Loss')\n",
    "        axes[1, 1].set_xlabel('Epoch')\n",
    "        axes[1, 1].set_ylabel('Loss')\n",
    "        axes[1, 1].set_title('Training Loss')\n",
    "        axes[1, 1].legend()\n",
    "        axes[1, 1].grid(True)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig('./checkpoints/visualizations/training_progress.png', dpi=150)\n",
    "        plt.show()\n",
    "        \n",
    "        print(f\"Best PSNR: {{df['psnr'].max():.4f}} at epoch {{df.loc[df['psnr'].idxmax(), 'epoch']}}\")\n",
    "        print(f\"Best SSIM: {{df['ssim'].max():.4f}} at epoch {{df.loc[df['ssim'].idxmax(), 'epoch']}}\")\n",
    "        print(f\"Best LPIPS: {{df['lpips'].min():.4f}} at epoch {{df.loc[df['lpips'].idxmin(), 'epoch']}}\")\n",
    "    \n",
    "    # 2. List all checkpoints\n",
    "    print(\"\\\\n Available checkpoints:\")\n",
    "    checkpoints_dir = Path(\"./checkpoints/by_epoch\")\n",
    "    for subdir in checkpoints_dir.iterdir():\n",
    "        if subdir.is_dir():\n",
    "            print(f\"\\\\n  {{subdir.name}}/\")\n",
    "            for cp in subdir.glob(\"*.pt\"):\n",
    "                print(f\"    - {{cp.name}}\")\n",
    "    \n",
    "    # 3. Show best models\n",
    "    print(\"\\\\n Best models:\")\n",
    "    best_dir = Path(\"./checkpoints/best_models\")\n",
    "    for metric_dir in best_dir.iterdir():\n",
    "        if metric_dir.is_dir():\n",
    "            metric_name = metric_dir.name.replace('by_', '')\n",
    "            best_files = list(metric_dir.glob(\"*.pt\"))\n",
    "            if best_files:\n",
    "                latest_best = max(best_files, key=lambda x: x.stat().st_mtime)\n",
    "                print(f\"  {{metric_name.upper()}}: {{latest_best.name}}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    visualize_training_progress()\n",
    "'''\n",
    "    \n",
    "    script_path = Path(\"./visualize_checkpoints.py\")\n",
    "    script_path.write_text(script_content)\n",
    "    \n",
    "    return script_path\n",
    "\n",
    "print(\"\\n Creating utility functions...\")\n",
    "log_file = create_training_log()\n",
    "viz_script = create_visualization_script()\n",
    "print(f\" Training log: {log_file}\")\n",
    "print(f\" Visualization script: {viz_script}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b03d713",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 276 lines from train.py\n",
      "Found safe_save_checkpoint at line 21\n",
      "Function spans lines 21 to 103\n",
      "üìÅ Backup saved as: train.py.backup_20251204_212548\n",
      "‚úÖ Successfully replaced safe_save_checkpoint with organized version!\n",
      "‚úì Organized checkpoint features confirmed\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell 8: FIXED VERSION with proper encoding handling\n",
    "def apply_organized_checkpoints():\n",
    "    \"\"\"Apply organized checkpoint system to train.py - FIXED FOR ENCODING\"\"\"\n",
    "    \n",
    "    # Read the current train.py with proper encoding\n",
    "    try:\n",
    "        with open('train.py', 'r', encoding='utf-8') as f:\n",
    "            lines = f.readlines()\n",
    "    except UnicodeDecodeError:\n",
    "        # Try other encodings\n",
    "        try:\n",
    "            with open('train.py', 'r', encoding='gbk') as f:\n",
    "                lines = f.readlines()\n",
    "        except:\n",
    "            with open('train.py', 'r', encoding='latin-1') as f:\n",
    "                lines = f.readlines()\n",
    "    \n",
    "    print(f\"Read {len(lines)} lines from train.py\")\n",
    "    \n",
    "    # Find the safe_save_checkpoint function\n",
    "    start_line = -1\n",
    "    for i, line in enumerate(lines):\n",
    "        if 'def safe_save_checkpoint' in line:\n",
    "            start_line = i\n",
    "            print(f\"Found safe_save_checkpoint at line {i+1}\")\n",
    "            break\n",
    "    \n",
    "    if start_line == -1:\n",
    "        print(\"‚ùå Could not find safe_save_checkpoint function\")\n",
    "        # Let's see what's actually there\n",
    "        for i, line in enumerate(lines[:50]):\n",
    "            print(f\"{i+1}: {line[:100]}\")\n",
    "        return False\n",
    "    \n",
    "    # Find where the function ends (look for next function definition or class)\n",
    "    end_line = start_line\n",
    "    in_function = True\n",
    "    for i in range(start_line + 1, len(lines)):\n",
    "        line = lines[i]\n",
    "        \n",
    "        # Check for next function/class definition (not indented)\n",
    "        if line.strip() and not line.startswith((' ', '\\t', ')', '}', ']')):\n",
    "            if line.strip().startswith(('def ', 'class ', '@')):\n",
    "                end_line = i - 1\n",
    "                break\n",
    "        \n",
    "        # Check for end of file\n",
    "        if i == len(lines) - 1:\n",
    "            end_line = i\n",
    "            break\n",
    "    \n",
    "    print(f\"Function spans lines {start_line+1} to {end_line+1}\")\n",
    "    \n",
    "    # Create the organized checkpoint function\n",
    "    organized_func = '''def safe_save_checkpoint(model, optim, scheduler, metrics_eval, metrics_train, path, global_rank):\n",
    "    \"\"\"Save checkpoints in organized structure\"\"\"\n",
    "    import os\n",
    "    import torch\n",
    "    import time\n",
    "    \n",
    "    if global_rank != 0:\n",
    "        return metrics_eval.get('valid_psnr', 0)\n",
    "    \n",
    "    epoch = metrics_train.get('epoch', 0)\n",
    "    psnr = metrics_eval.get('valid_psnr', 0)\n",
    "    timestamp = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    # Ensure base directory exists\n",
    "    base_path = path if not path.endswith(('.pt', '.pth')) else os.path.dirname(path)\n",
    "    if not base_path:\n",
    "        base_path = \"./checkpoints\"\n",
    "    os.makedirs(base_path, exist_ok=True)\n",
    "    \n",
    "    # 1. Save to by_epoch/ folder (every 10 epochs)\n",
    "    if epoch % 10 == 0 or epoch == metrics_train.get('total_epochs', 1000) - 1:\n",
    "        epoch_dir = os.path.join(base_path, \"by_epoch\")\n",
    "        os.makedirs(epoch_dir, exist_ok=True)\n",
    "        epoch_path = os.path.join(epoch_dir, f\"epoch_{epoch:03d}_psnr_{psnr:.2f}.pt\")\n",
    "        \n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optim.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict() if scheduler else None,\n",
    "            'metrics_eval': metrics_eval,\n",
    "            'metrics_train': metrics_train,\n",
    "            'timestamp': timestamp,\n",
    "            'psnr': psnr\n",
    "        }, epoch_path)\n",
    "        \n",
    "        print(f\"üìÅ Epoch checkpoint saved: {epoch_path}\")\n",
    "    \n",
    "    # 2. Save latest model\n",
    "    latest_dir = os.path.join(base_path, \"latest\")\n",
    "    os.makedirs(latest_dir, exist_ok=True)\n",
    "    latest_path = os.path.join(latest_dir, \"latest_checkpoint.pt\")\n",
    "    \n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optim.state_dict(),\n",
    "        'scheduler_state_dict': scheduler.state_dict() if scheduler else None,\n",
    "        'metrics_eval': metrics_eval,\n",
    "        'metrics_train': metrics_train,\n",
    "        'timestamp': timestamp,\n",
    "        'psnr': psnr\n",
    "    }, latest_path)\n",
    "    \n",
    "    # 3. Track and save best model\n",
    "    best_dir = os.path.join(base_path, \"best_models\")\n",
    "    os.makedirs(best_dir, exist_ok=True)\n",
    "    \n",
    "    # Check current best PSNR\n",
    "    best_psnr_file = os.path.join(best_dir, \"best_psnr.txt\")\n",
    "    best_psnr = 0\n",
    "    \n",
    "    if os.path.exists(best_psnr_file):\n",
    "        try:\n",
    "            with open(best_psnr_file, 'r') as f:\n",
    "                best_psnr = float(f.read().strip())\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    # Save if this is the best model\n",
    "    if psnr > best_psnr:\n",
    "        # Update best PSNR file\n",
    "        with open(best_psnr_file, 'w') as f:\n",
    "            f.write(str(psnr))\n",
    "        \n",
    "        # Save best model\n",
    "        best_path = os.path.join(best_dir, f\"best_psnr_{psnr:.2f}_epoch_{epoch}.pt\")\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optim.state_dict(),\n",
    "            'metrics_eval': metrics_eval,\n",
    "            'metrics_train': metrics_train,\n",
    "            'timestamp': timestamp,\n",
    "            'psnr': psnr,\n",
    "            'is_best': True\n",
    "        }, best_path)\n",
    "        \n",
    "        print(f\"üèÜ NEW BEST MODEL! PSNR: {psnr:.4f} saved to: {best_path}\")\n",
    "    \n",
    "    return psnr'''\n",
    "    \n",
    "    # Replace the function\n",
    "    new_lines = lines[:start_line] + [organized_func + '\\n'] + lines[end_line + 1:]\n",
    "    \n",
    "    # Backup original\n",
    "    import shutil\n",
    "    timestamp = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "    backup_name = f'train.py.backup_{timestamp}'\n",
    "    shutil.copy2('train.py', backup_name)\n",
    "    print(f\"üìÅ Backup saved as: {backup_name}\")\n",
    "    \n",
    "    # Write new content with UTF-8 encoding\n",
    "    with open('train.py', 'w', encoding='utf-8') as f:\n",
    "        f.writelines(new_lines)\n",
    "    \n",
    "    print(\"‚úÖ Successfully replaced safe_save_checkpoint with organized version!\")\n",
    "    \n",
    "    # Quick verification\n",
    "    with open('train.py', 'r', encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "        if 'by_epoch' in content and 'best_models' in content:\n",
    "            print(\"‚úì Organized checkpoint features confirmed\")\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è Warning: Organized features not found - check the file\")\n",
    "    \n",
    "    return True\n",
    "\n",
    "# Import time for timestamp\n",
    "import time\n",
    "\n",
    "# Run the patch\n",
    "apply_organized_checkpoints()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1da067f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Organized Checkpoint Features Check:\n",
      "  ‚úì by_epoch folder\n",
      "  ‚úì latest folder\n",
      "  ‚úì best_models folder\n",
      "  ‚úì best_psnr.txt tracking\n",
      "  ‚úì organized print statements\n",
      "\n",
      "üìè New function is 90 lines long\n",
      "üìÅ Backup file exists\n"
     ]
    }
   ],
   "source": [
    "# Quick Verification Commands:\n",
    "\n",
    "# 1. Check if the organized function is there\n",
    "with open('train.py', 'r', encoding='utf-8') as f:\n",
    "    content = f.read()\n",
    "    \n",
    "# Check for key features\n",
    "features_to_check = {\n",
    "    'by_epoch folder': '\"by_epoch\"' in content,\n",
    "    'latest folder': '\"latest\"' in content,\n",
    "    'best_models folder': '\"best_models\"' in content,\n",
    "    'best_psnr.txt tracking': 'best_psnr.txt' in content,\n",
    "    'organized print statements': 'üìÅ Epoch checkpoint' in content,\n",
    "}\n",
    "\n",
    "print(\"‚úÖ Organized Checkpoint Features Check:\")\n",
    "for feature, present in features_to_check.items():\n",
    "    status = \"‚úì\" if present else \"‚úó\"\n",
    "    print(f\"  {status} {feature}\")\n",
    "\n",
    "# 2. Count lines of the new function\n",
    "import re\n",
    "match = re.search(r'def safe_save_checkpoint\\(.*?\\):.*?(?=\\n\\S|\\Z)', content, re.DOTALL)\n",
    "if match:\n",
    "    func_lines = match.group(0).count('\\n')\n",
    "    print(f\"\\nüìè New function is {func_lines} lines long\")\n",
    "    \n",
    "# 3. Check backup exists\n",
    "import os\n",
    "if os.path.exists('train.py.backup_20251204_212548'):\n",
    "    print(\"üìÅ Backup file exists\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_rsblur",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
