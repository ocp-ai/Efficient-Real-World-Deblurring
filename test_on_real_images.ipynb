{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1751039",
   "metadata": {},
   "source": [
    "## Cell 1: Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a296049",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies if you are in Colab or haven't installed them yet\n",
    "# !pip install tifffile imagecodecs opencv-python matplotlib\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tifffile\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm # Progress bar\n",
    "\n",
    "# Setup Device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"✅ Using device: {device}\")\n",
    "\n",
    "# Display settings for Jupyter\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = [12, 6]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "553e55b5",
   "metadata": {},
   "source": [
    "## Cell 2: Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a286ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- USER CONFIGURATION ---\n",
    "CHECKPOINT_PATH = './checkpoints/best_models/best_psnr.pt'  # Update this path!\n",
    "INPUT_DIR = './real_microscope_images'                        # Update this path!\n",
    "OUTPUT_DIR = './results_jupyter'                              # Where to save results\n",
    "\n",
    "# Model Architecture Config (Must match your training exactly)\n",
    "MODEL_CONFIG = {\n",
    "    'img_channels': 3,\n",
    "    'width': 16,\n",
    "    'middle_blk_num': 1,\n",
    "    'enc_blk_nums': [1, 1, 1, 14],\n",
    "    'dec_blk_nums': [1, 1, 1, 1]\n",
    "}\n",
    "\n",
    "# Ensure output directory exists\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b8a4d46",
   "metadata": {},
   "source": [
    "## Cell 3: Model Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e14eddc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_nafnet(checkpoint_path, config, device):\n",
    "    \"\"\"Load the trained NAFNet model.\"\"\"\n",
    "    print(f\"⏳ Loading model from: {checkpoint_path}\")\n",
    "    \n",
    "    # Import your local architecture\n",
    "    try:\n",
    "        from archs import create_model\n",
    "    except ImportError:\n",
    "        raise ImportError(\"Could not import 'archs'. Make sure archs.py is in the current directory!\")\n",
    "\n",
    "    # Create Model\n",
    "    # We add 'name': 'NAFNet' assuming create_model expects it\n",
    "    model, _, _ = create_model({'name': 'NAFNet', **config}, \n",
    "                               local_rank=0, global_rank=0)\n",
    "    \n",
    "    # Load Weights\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "    \n",
    "    if 'model_state_dict' in checkpoint:\n",
    "        state_dict = checkpoint['model_state_dict']\n",
    "    elif 'model' in checkpoint:\n",
    "        state_dict = checkpoint['model']\n",
    "    else:\n",
    "        state_dict = checkpoint\n",
    "        \n",
    "    model.load_state_dict(state_dict)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    print(\"✅ Model loaded successfully!\")\n",
    "    return model\n",
    "\n",
    "# Load the model now\n",
    "model = load_nafnet(CHECKPOINT_PATH, MODEL_CONFIG, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e60302",
   "metadata": {},
   "source": [
    "## Cell 4: Inference Engine (The Core Logic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ec6447",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_and_visualize(model, img_path, save_dir, device):\n",
    "    filename = os.path.basename(img_path)\n",
    "    \n",
    "    # --- 1. Load Image (Robust TIF Handling) ---\n",
    "    if img_path.lower().endswith(('.tif', '.tiff')):\n",
    "        img_data = tifffile.imread(img_path)\n",
    "        # Handle 16-bit\n",
    "        if img_data.dtype in (np.uint16, np.int16, np.uint32, np.int32):\n",
    "            max_val = np.iinfo(img_data.dtype).max\n",
    "            img_np = (img_data / max_val * 255.0).astype(np.uint8)\n",
    "        else:\n",
    "            img_np = img_data.astype(np.uint8)\n",
    "            \n",
    "        # Ensure RGB\n",
    "        if img_np.ndim == 2:\n",
    "            img_np = cv2.cvtColor(img_np, cv2.COLOR_GRAY2RGB)\n",
    "        elif img_np.shape[2] == 1:\n",
    "            img_np = cv2.cvtColor(img_np, cv2.COLOR_GRAY2RGB)\n",
    "            \n",
    "        img_pil = Image.fromarray(img_np).convert('RGB')\n",
    "    else:\n",
    "        img_pil = Image.open(img_path).convert('RGB')\n",
    "\n",
    "    # --- 2. Preprocessing (Assuming [0, 1] Range) ---\n",
    "    # NOTE: We DO NOT use Normalize(0.5, 0.5) based on NAFNet standard practice\n",
    "    transform = transforms.ToTensor()\n",
    "    img_tensor = transform(img_pil).unsqueeze(0).to(device)\n",
    "\n",
    "    # --- 3. Inference ---\n",
    "    with torch.no_grad():\n",
    "        output_tensor = model(img_tensor)\n",
    "\n",
    "    # --- 4. Post-processing ---\n",
    "    # Convert tensor (0.0 - 1.0) back to numpy (0 - 255)\n",
    "    output_np = output_tensor.squeeze().cpu().numpy().transpose(1, 2, 0)\n",
    "    output_np = np.clip(output_np * 255.0, 0, 255).astype(np.uint8)\n",
    "\n",
    "    # --- 5. Save Result ---\n",
    "    save_path = os.path.join(save_dir, f\"deblurred_{filename.split('.')[0]}.png\")\n",
    "    # OpenCV uses BGR, so convert RGB -> BGR for saving\n",
    "    cv2.imwrite(save_path, cv2.cvtColor(output_np, cv2.COLOR_RGB2BGR))\n",
    "    \n",
    "    return img_pil, output_np, save_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c01dc7c",
   "metadata": {},
   "source": [
    "## Cell 5: Run and Display Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "769dfcd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of images\n",
    "image_extensions = ('.png', '.jpg', '.jpeg', '.tif', '.tiff', '.bmp')\n",
    "all_images = [f for f in os.listdir(INPUT_DIR) if f.lower().endswith(image_extensions)]\n",
    "\n",
    "print(f\"Found {len(all_images)} images to process.\")\n",
    "\n",
    "# Process up to 5 images for demonstration (remove [:5] to process all)\n",
    "for img_name in tqdm(all_images[:5]): \n",
    "    img_path = os.path.join(INPUT_DIR, img_name)\n",
    "    \n",
    "    try:\n",
    "        # Run inference\n",
    "        original, result, save_loc = process_and_visualize(model, img_path, OUTPUT_DIR, device)\n",
    "        \n",
    "        # --- Visualization in Jupyter ---\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "        \n",
    "        # Original\n",
    "        axes[0].imshow(original)\n",
    "        axes[0].set_title(f\"Original: {img_name}\")\n",
    "        axes[0].axis('off')\n",
    "        \n",
    "        # Deblurred\n",
    "        axes[1].imshow(result)\n",
    "        axes[1].set_title(\"NAFNet Restoration\")\n",
    "        axes[1].axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        print(f\"Saved to: {save_loc}\\n\" + \"-\"*50)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error processing {img_name}: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_rsblur",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
